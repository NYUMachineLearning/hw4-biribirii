---
title: "ML-HW4"
author: "Brian Chang"
date: "2019/10/29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load libraries
```{r load relevant libraries, include=FALSE}
library(tidyverse)
library(caret)
library(randomForest)
library(mlbench)
library(glmnet)
library(datasets)
library(dplyr)
library(MASS)
library(nnet)
library(e1071)
library(AER)
```


## Homework

1. Compare the most important features from at least 2 different classes of feature selection methods covered in this tutorial with any reasonable machine learning dataset from mlbench. Do these feature selection methods provide similar results? 

2. Attempt a feature selection method not covered in this tutorial (backward elimination, forward propogation, etc.)

## Load Dataset
```{r data}
data(PimaIndiansDiabetes)
indian <- as.data.frame(PimaIndiansDiabetes)
indian_num <- indian[,1:8]

indian_num = transform(indian_num, pregnant = as.numeric(pregnant), 
                       glucose = as.numeric(glucose),
                       pressure = as.numeric(pressure),
                       triceps = as.numeric(triceps),
                       insulin = as.numeric(insulin),
                       mass = as.numeric(mass),
                       pedigree = as.numeric(pedigree),
                       age = as.numeric(age))
```

## Filter Method (Pearson's Correlation)
* When threshold is set at 0.7, no variables were selected for.
* Lowered threshold to 0.5, which selects for only "age."
```{r correlation}
# calculate correlation
correlation_matrix = cor(indian_num)

# correlation plot
library(corrplot)
corrplot(correlation_matrix, order = "hclust")

# keep correlation >=0.7
highly_correlated <- colnames(indian)[findCorrelation(correlation_matrix, cutoff = 0.5, verbose = TRUE)]
highly_correlated
```

## Wrapper Method (RFE)
* This method selects "age" as one of the top 3 variables. However, there are 2 others - "glucose" and "mass"
* This demonstrates similar results to the filter method as "age" is one of our selected variables.
```{r RFE}
indian_num[is.na(indian_num)] = 0

#define the control 
control = rfeControl(functions = caretFuncs, number = 2)

# run the RFE algorithm
results = rfe(indian[,1:8], indian[,9], sizes = c(2,3,5), rfeControl = control, method = "svmRadial")
# sizes is number of features you want to pull out

results
results$variables

plot(results, type=c('g','o'))

predictors(results)
```


## Simulated Annealing
* We will fit a random forest model and run 10 iterations of the algorithm.
  + Only 10 iterations will be run for computational intensity.
* We see that the top 3 selected variables are - "glucose", "age", and "insulin" and "mass" being tied for third. 
* The variables selected here are similar to the wrapper method earlier.

```{r SA}
sa_ctrl <- safsControl(functions = rfSA,
                       method = "repeatedcv",
                       repeats = 5,
                       improve = 50)
set.seed(10)
rf_sa <- safs(x = indian_num, y = indian[,9],
              iters = 10,
              safsControl = sa_ctrl)
rf_sa
```

